{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/profjsb/python-seminar/blob/master/DataFiles_and_Notebooks/07_machine_learning_2/02_neural_nets_with_keras.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Frameworks\n",
    "\n",
    "AY 250 (UC Berkeley; 2012-2022)\n",
    "\n",
    "<div class=\"alert alert-info\">sklearn is not built for deep/complex networks such as required in covnets (as we'll see later on). We must go to specialized software (and potentially specialized hardware)</div>\n",
    "\n",
    "Almost all frameworks written in low-level C++/C with Python (or other scripting bindings)\n",
    "\n",
    "### Low-level frameworks\n",
    "\n",
    "   - Tensorflow (Google) Nov 2015. See https://www.tensorflow.org/api_docs/python/tf\n",
    "   - pytorch (Python). https://pytorch.org/docs/stable/index.html\n",
    "   - Theano\n",
    "   - Caffe (Berkeley)\n",
    "   - Torch (Lua)\n",
    "   - CNTK (Microsoft)\n",
    "   - Chainer\n",
    "   - PaddlePaddle (Baidu) Aug 2016\n",
    "   \n",
    "### High-level frameworks (Python)\n",
    "\n",
    "   - Keras (atop Tensorflow, Theano) - https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "   - Skorch - scikit-learn compatible neural network library that wraps PyTorch (https://github.com/skorch-dev/skorch)\n",
    "   - FastAI: https://docs.fast.ai/\n",
    "   - PyTorch Lightning (https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "   \n",
    "<img src=\"https://pbs.twimg.com/media/DX0lfBNU8AEs8KG.png:large\" width=\"75%\">\n",
    "Source: https://twitter.com/fchollet/status/971863128341323776\n",
    "\n",
    "See also: https://www.reddit.com/r/MachineLearning/comments/esrtxu/d_which_pytorchcompatible_training_abstraction/\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">TensorFlow is the platform of choice for deep learning in the research community. These are deep learning framework mentions on arXiv over the past 3 months <img src=\"https://pbs.twimg.com/media/DXy_uc0VAAAIhKG.jpg:small\">\n",
    "\n",
    "&mdash; Fran√ßois Chollet (@fchollet) <a href=\"https://twitter.com/fchollet/status/971863128341323776?ref_src=twsrc%5Etfw\">March 8, 2018</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "\n",
    "see also: https://github.com/mbadry1/Top-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.asimovinstitute.org/wp-content/uploads/2016/09/neuralnetworks.png\">\n",
    "\n",
    "Source: http://www.asimovinstitute.org/neural-network-zoo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example (from Josh's student work): \n",
    "\n",
    "<img src=\"https://github.com/profjsb/deepCR/raw/master/imgs/network.png\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/profjsb/deepCR/master/imgs/postage-sm.jpg\">\n",
    "\n",
    "\"deepCR: Deep Learning Based Cosmic Ray Removal for Astronomical Images\"\n",
    "https://github.com/profjsb/deepCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "\n",
    "\"Use Keras if you need a deep learning library that:\n",
    "\n",
    "Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "Runs seamlessly on CPU and GPU.\"\n",
    "\n",
    "-- keras.io\n",
    "\n",
    "<img src=\"https://www.digikey.com/maker-media/6c3d4f5f-98e0-4104-ad8c-fb0b47000109\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the California housing data as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "cal_data = datasets.fetch_california_housing()\n",
    "X = cal_data['data']   # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_features = train_X.shape[1]\n",
    "print(f'number of input features = {num_input_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Print keras version\n",
    "print(tensorflow.keras.__version__)\n",
    "\n",
    "from tensorflow import keras\n",
    "print(f\"backend={keras.backend.backend()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_clf():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_shape=(num_input_features,), \n",
    "                      activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(32,  activation=\"relu\", kernel_initializer='random_uniform', name=\"dan\"))\n",
    "    model.add(Dense(10,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer='random_uniform'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae',\"mse\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_clf()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, \n",
    "                   show_layer_names=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"model_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "clf = KerasRegressor(model=nn_clf, batch_size=32, epochs=50)\n",
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well did we do?\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_Y, clf.predict(test_X)) ; print(\"MSE\",mse)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"NN Regression Residuals - MSE = %.3f\" % mse)\n",
    "plt.scatter(test_Y,clf.predict(test_X),alpha=0.4,s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2,5],[0.2,5],c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complete Example\n",
    "\n",
    "We want to train and make some decisions of when to stop based on `validation` data. Ultimately, we'd like to see how well our model would do on truly new data (`test`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage, valid_percentage, test_percentage = (0.90, 0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rnd = np.random.RandomState(42)\n",
    "\n",
    "# make an array with the indices of all the rows in the dataset\n",
    "ind_arr = np.arange(X.shape[0])\n",
    "rnd.shuffle(ind_arr)\n",
    "\n",
    "train_ind, tmp = train_test_split(ind_arr, train_size=train_percentage, random_state=rnd)\n",
    "valid_ind, test_ind = \\\n",
    "      train_test_split(tmp, train_size=valid_percentage/(valid_percentage + test_percentage), \n",
    "                               random_state=rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that we're getting all the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.array(sorted(list(train_ind) + list(valid_ind) + list(test_ind)))  == \\\n",
    "              sorted(ind_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind.shape, test_ind.shape, valid_ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "\n",
    "train_X = X[train_ind]\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test, validation data\n",
    "test_X = scaler.transform(X[test_ind])\n",
    "valid_X = scaler.transform(X[valid_ind])\n",
    "\n",
    "train_y = Y[train_ind] ; test_y = Y[test_ind] ; valid_y = Y[valid_ind]\n",
    "\n",
    "assert train_y.shape[0] == train_X.shape[0]\n",
    "assert test_y.shape[0] == test_X.shape[0]\n",
    "assert valid_y.shape[0] == valid_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(num_input_features,), \n",
    "                  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "model.add(Dense(32,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "model.add(Dense(5,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model.compile` method in `keras` has a number of input parameters:\n",
    "\n",
    "```python\n",
    "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "```\n",
    "Usually, you'll set the `optimizer`, `loss`, and `metrics`.\n",
    "\n",
    "https://keras.io/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae',\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger, \\\n",
    "                                                ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "try:\n",
    "    os.mkdir('nn_results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "batch_size=64\n",
    "num_epochs = 200\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "\n",
    "# define path to save model\n",
    "model_path = f'nn_results/ay250_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "# Tensorboard is a project which can ingest learning logs for interactive display...\n",
    "# more on that later.\n",
    "tb = TensorBoard(log_dir='nn_results', histogram_freq=0,\n",
    "                 write_graph=True, \n",
    "                 write_grads=False, \n",
    "                 write_images=False, \n",
    "                 embeddings_freq=0, \n",
    "                 embeddings_layer_names=None, \n",
    "                 embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mse', factor=0.75,\n",
    "                              patience=3, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mse', min_delta=0.001, patience=10, \\\n",
    "                          verbose=1, mode='auto')\n",
    "\n",
    "model_check = ModelCheckpoint(model_path,\n",
    "        monitor='val_mse', \n",
    "        save_best_only=True, \n",
    "        mode='min',\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model.fit` method:\n",
    "\n",
    "```python\n",
    "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "    has_gpu = False\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    has_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_X, y=train_y,\n",
    "               epochs=num_epochs,\n",
    "               validation_data=(valid_X, valid_y),\n",
    "               verbose=1, shuffle=True,\n",
    "               callbacks=[csv_logger, earlystop, model_check, tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nn_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the history of the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastest_log_file = !ls -t1 nn_results/training* | head -1\n",
    "hist_df = pd.read_csv(lastest_log_file[0])\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also available in the return value from `.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.title(\"Training History\")\n",
    "plt.plot(hist_df.index + 1,hist_df[\"val_mse\"] ,alpha=0.4, label=\"validation\")\n",
    "plt.plot(hist_df.index + 1,hist_df[\"mse\"] ,alpha=0.4, label=\"training\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.loglog()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'h5py==2.10.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best model\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = load_model(\"nn_results/ay250_nn_2022-03-26T01:25.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well did we do?\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_y, pred_y[:,0]); print(\"MSE\",mse)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"NN Regression Residuals - MSE = %.3f\" % mse)\n",
    "plt.scatter(test_y,pred_y[:,0] ,alpha=0.4,s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2,5],[0.2,5],c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do compared to the validation and training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in hyperparameter optimization, you could try out Weights & Biases for Keras (https://wandb.ai/site/articles/intro-to-keras-with-weights-biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Training on a GPU (on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "    has_gpu = False\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    has_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build, compile, and fit the model within the GPU context:\n",
    "```python\n",
    "with tf.device('/device:GPU:0'):\n",
    "  model = Sequential()\n",
    "  model.add(Dense(32, input_shape=(num_input_features,), \n",
    "                    activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "  ...\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae',\"mse\"])\n",
    "  \n",
    "  history = model.fit(x=train_X, y=train_y,\n",
    "               epochs=num_epochs,\n",
    "               validation_data=(valid_X, valid_y),\n",
    "               verbose=1, shuffle=True,\n",
    "               callbacks=[csv_logger, earlystop, model_check, tb])\n",
    "```\n",
    "\n",
    "To use the TPU (way overkill for this project) see https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy     : 1.20.3\n",
      "autopep8  : 1.6.0\n",
      "sklearn   : 0.0\n",
      "seaborn   : 0.11.2\n",
      "json      : 2.0.9\n",
      "matplotlib: 3.5.1\n",
      "tensorflow: 2.8.0\n",
      "keras     : 2.8.0\n",
      "pandas    : 1.3.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
