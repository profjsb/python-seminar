{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2025f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../00_AdvancedPythonConcepts/talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "pip install --upgrade pydot-ng\n",
    "pip install python-resize-image\n",
    "conda update sklearn\n",
    "\n",
    "# TensorFlow & Keras\n",
    "conda update -f -c conda-forge keras tensorflow protobuf\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and Unsupervised Learning\n",
    "\n",
    "Python for Data Science (AY250: UC Berkeley 2016-2018; J. Bloom)\n",
    "\n",
    "<u>Last Week:</u>\n",
    "\n",
    "- supervised\n",
    "    - regression\n",
    "    - classification\n",
    "- feature engineering, ML pipelines\n",
    "- pipelines: hyperparamter optimization, feature selection\n",
    "\n",
    "<u>This Week:</u>\n",
    "\n",
    "- Deep learning\n",
    "   - simple networks\n",
    "   - complex networks for image analysis\n",
    "\n",
    "- Unsupervised learning\n",
    "   - clustering\n",
    "   - outlier detection\n",
    "   - manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "\n",
    "### What is DL?\n",
    "\n",
    "   - A set of methodologies for doing machine learning\n",
    "   - Collection/composition of simple mathematical functions whose parameterization is *learned* by passing over the data\n",
    "   - Modern version of \"artificial neural networks\"\n",
    "   \n",
    "### Why do people like it?\n",
    "   - It's \"inspired\" by how the brain is thought to work, so it *feels* like a natural approach. \n",
    "   - It works. Amazingly well. In a growing number of use cases.\n",
    "   - It's composeable, so it's \"easy\" to understand each piece.\n",
    "   - Featurizes + learns on \"raw\" data.\n",
    "   - Timely: It's tractable with the data/problems we have and the compute power we have access to.\n",
    "   - New shiny object with codebases getting commoditized (read: easier and easier to use...and free)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do people dislike it?\n",
    "\n",
    "   - Decades of hype\n",
    "   - It's considered a black box in a lot of ways\n",
    "   - It can take a ninja to get it right\n",
    "   - It's expensive to run/learn a model\n",
    "   - Not natively adapted to heterogenous data and certain types of learning\n",
    "   - Not the approach of choice for small/medium data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://fastml.com/images/ai/new_navy_device_learns_by_doing.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"Neuron\" (\"Perceptron\")\n",
    "<img src=\"https://www.evernote.com/l/AUVUbm0I38pMWbUhfC0VUZv7qxxguDOy64QB/image.png\">\n",
    "Source: http://www.wsdm-conference.org/2016/slides/WSDM2016-Jeff-Dean.pdf\n",
    "\n",
    "<img width=\"50%\" src=\"http://www.webpages.ttu.edu/dleverin/neural_network/fig_3p6_neurons_NEURON4.jpg\">\n",
    "Source: http://www.webpages.ttu.edu/dleverin/neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is to *learn* the weights $w_i$ given the data. This is done as such:\n",
    "\n",
    "  1. Initialization:\n",
    "      - Set the transfer (e.g. sum) & activation (sigmoid) functions you want to use.\n",
    "      - randomly assign the weights (with some probability distribution)\n",
    "  2. For each instance $i$, run your input $\\vec x_i$ through the network with current weights to get the current output.\n",
    "  3. Determine $\\Delta$ how far off the current output is from the true output/labels.\n",
    "  4. Update the weights by taking the gradient of the activation at $\\vec x_i$ and multiplying by $\\Delta$.\n",
    "  5. Repeat steps 2--5 until you hit a stopping criteria.\n",
    "  \n",
    "This process is an optimization and is called **\"Back Propogation\"** and, if your activation function is differentiable, it's basically a form of gradient descent and reduces to doing simple linear algebra to find the optimimal weights given the data.  It was first presented by Rumelhart, Hinton, Williams ([Nature, 1986](http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Network Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output label\n",
    "Y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imiloainf.files.wordpress.com/2013/11/activation_funcs1.png\" width=\"50%\">\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transfer(wx):\n",
    "    \" how to aggregate the weighted inputs \"\n",
    "    return np.sum(wx,axis=1)\n",
    "\n",
    "def activation(twx,func=\"ReLU\",derivative=False):\n",
    "    \" how to treat the sum of the weighted input \"\n",
    "    if func == \"ReLU\":\n",
    "        if derivative:\n",
    "            return np.array([0 if x <= 0 else 1 for x in twx])\n",
    "        return np.array([max(0,x) for x in twx])\n",
    "    \n",
    "    if func == \"sigmoid\":\n",
    "        if derivative:\n",
    "            return np.array([x*(1-x) for x in twx])\n",
    "        return np.array([1/(1+np.exp(-x)) for x in twx])\n",
    "    \n",
    "    if func == \"tanh\":\n",
    "        if derivative:\n",
    "            np.array([1 - (np.tanh(x))**2 for x in twx])\n",
    "        return np.array([np.tanh(x) for x in twx])\n",
    "    \n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "weights_initial = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms_error = {\"tanh\": [], \"sigmoid\": []}\n",
    "\n",
    "for func in [\"tanh\", \"sigmoid\"]:\n",
    "\n",
    "    weights = weights_initial.copy()\n",
    "\n",
    "    for _ in range(10000):\n",
    "        # forward propagation\n",
    "        layer0 = X\n",
    "        sum_of_weighted_X = transfer(layer0*weights.T)\n",
    "        layer1 = activation(sum_of_weighted_X,func=func)\n",
    "\n",
    "        # how much did we miss?\n",
    "        layer1_error = Y.T - layer1\n",
    "\n",
    "        rms_error[func].append(np.sqrt((layer1_error**2).sum()))\n",
    "        # multiply how much we missed by the\n",
    "        # slope of the activation at the values in layer1\n",
    "        layer1_delta = layer1_error * activation(layer1,derivative=True)\n",
    "\n",
    "        weights += np.dot(layer1_delta,layer0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.plot(rms_error[\"sigmoid\"],label=\"sigmoid\")\n",
    "plt.plot(rms_error[\"tanh\"],label=\"tanh\")\n",
    "\n",
    "plt.xscale(\"symlog\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"RMS Error\")\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.legend()\n",
    "plt.title(\"Single Layer NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we updated the weights at each pass by using all the instances (this is called \"batch learning\"). There are speed ups (but generally noisier learning) by randomly choosing a subset of the data at each iteration (\"stochastic learning\").\n",
    "\n",
    "See [LeCun, Bottou, Orr, & Muller 1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Networks\n",
    "\n",
    "Multilayer networks are not really any different. They have more weights to learn but they may also represent more complex models. Backpropogation optimization still works, this time by using the chain rule. That is, optimization is multi-step but it's  local to individual layers (this makes the problem tractable).\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" width=\"50%\">\n",
    "\n",
    "The above network is said to have a hidden layer, which is neither an input nor an output layer.\n",
    "\n",
    "In sklearn there are a few solver for backpropogation optimization:\n",
    "  - ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "  - ‘sgd’ refers to stochastic gradient descent.\n",
    "  - ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(solver='lbfgs',activation=\"tanh\",\n",
    "                    hidden_layer_sizes=(5,2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output label\n",
    "Y = np.array([0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run draw_nn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_MLP_model(fig.gca(),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iterations:\", clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(np.array([1,0,1]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(np.array([1,1,0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "\n",
    "# output label\n",
    "Y = [[\"sad\",\"mouse\"],[\"happy\",\"dog\"],[\"sad\",\"mouse\"],[\"happy\",\"cat\"]]\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y1 = mlb.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs',activation=\"tanh\",\n",
    "                    hidden_layer_sizes=(15,10,6), random_state=1)\n",
    "clf.fit(X, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_MLP_model(fig.gca(),clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newx = np.array([0,0,1]).reshape(1,3)\n",
    "clf.predict(newx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cal_house = datasets.california_housing\n",
    "cal_data = cal_house.fetch_california_housing()\n",
    "X = cal_data['data']   # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(activation=\"tanh\",alpha=0.1,solver='sgd',\n",
    "                   nesterovs_momentum=False, learning_rate_init=0.2,\n",
    "                   hidden_layer_sizes=(20,20,20,5), random_state=1, max_iter = 400)\n",
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(test_Y,clf.predict(test_X),alpha=0.3,s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(test_X,test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout \n",
    "\n",
    "Build a multi-layer NN classifier for the Iris dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[multi-layer neural nets in the Browser]( http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.65948&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
